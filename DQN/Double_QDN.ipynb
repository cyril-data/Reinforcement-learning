{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-24T14:04:42.065Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: 11, avg loss: 0.000, eps: 0.994\n",
      "Episode: 1, Reward: 10, avg loss: 0.000, eps: 0.989\n",
      "Episode: 2, Reward: 20, avg loss: 0.000, eps: 0.978\n",
      "Episode: 3, Reward: 30, avg loss: 0.000, eps: 0.964\n",
      "Episode: 4, Reward: 24, avg loss: 0.257, eps: 0.952\n",
      "Episode: 5, Reward: 19, avg loss: 0.862, eps: 0.942\n",
      "Episode: 6, Reward: 16, avg loss: 0.869, eps: 0.934\n",
      "Episode: 7, Reward: 33, avg loss: 0.954, eps: 0.919\n",
      "Episode: 8, Reward: 19, avg loss: 1.145, eps: 0.910\n",
      "Episode: 9, Reward: 13, avg loss: 1.535, eps: 0.904\n",
      "Episode: 10, Reward: 10, avg loss: 1.502, eps: 0.899\n",
      "Episode: 11, Reward: 12, avg loss: 2.176, eps: 0.893\n",
      "Episode: 12, Reward: 33, avg loss: 3.043, eps: 0.878\n",
      "Episode: 13, Reward: 9, avg loss: 4.733, eps: 0.874\n",
      "Episode: 14, Reward: 11, avg loss: 4.771, eps: 0.869\n",
      "Episode: 15, Reward: 37, avg loss: 6.326, eps: 0.852\n",
      "Episode: 16, Reward: 13, avg loss: 7.279, eps: 0.846\n",
      "Episode: 17, Reward: 10, avg loss: 6.540, eps: 0.842\n",
      "Episode: 18, Reward: 11, avg loss: 15.645, eps: 0.837\n",
      "Episode: 19, Reward: 12, avg loss: 17.096, eps: 0.832\n",
      "Episode: 20, Reward: 12, avg loss: 13.663, eps: 0.826\n",
      "Episode: 21, Reward: 26, avg loss: 12.763, eps: 0.815\n",
      "Episode: 22, Reward: 18, avg loss: 21.812, eps: 0.808\n",
      "Episode: 23, Reward: 16, avg loss: 21.383, eps: 0.801\n",
      "Episode: 24, Reward: 12, avg loss: 15.166, eps: 0.796\n",
      "Episode: 25, Reward: 17, avg loss: 13.130, eps: 0.789\n",
      "Episode: 26, Reward: 18, avg loss: 17.203, eps: 0.781\n",
      "Episode: 27, Reward: 36, avg loss: 19.197, eps: 0.767\n",
      "Episode: 28, Reward: 11, avg loss: 13.864, eps: 0.763\n",
      "Episode: 29, Reward: 11, avg loss: 15.529, eps: 0.758\n",
      "Episode: 30, Reward: 12, avg loss: 13.366, eps: 0.753\n",
      "Episode: 31, Reward: 21, avg loss: 12.916, eps: 0.745\n",
      "Episode: 32, Reward: 18, avg loss: 10.941, eps: 0.738\n",
      "Episode: 33, Reward: 8, avg loss: 13.200, eps: 0.735\n",
      "Episode: 34, Reward: 18, avg loss: 11.271, eps: 0.728\n",
      "Episode: 35, Reward: 9, avg loss: 12.368, eps: 0.725\n",
      "Episode: 36, Reward: 11, avg loss: 12.133, eps: 0.720\n",
      "Episode: 37, Reward: 14, avg loss: 10.346, eps: 0.715\n",
      "Episode: 38, Reward: 10, avg loss: 9.600, eps: 0.711\n",
      "Episode: 39, Reward: 9, avg loss: 10.304, eps: 0.708\n",
      "Episode: 40, Reward: 21, avg loss: 7.356, eps: 0.700\n",
      "Episode: 41, Reward: 33, avg loss: 7.868, eps: 0.688\n",
      "Episode: 42, Reward: 8, avg loss: 4.604, eps: 0.685\n",
      "Episode: 43, Reward: 19, avg loss: 9.366, eps: 0.679\n",
      "Episode: 44, Reward: 17, avg loss: 9.348, eps: 0.673\n",
      "Episode: 45, Reward: 19, avg loss: 5.007, eps: 0.666\n",
      "Episode: 46, Reward: 12, avg loss: 5.769, eps: 0.662\n",
      "Episode: 47, Reward: 13, avg loss: 9.658, eps: 0.657\n",
      "Episode: 48, Reward: 21, avg loss: 8.040, eps: 0.650\n",
      "Episode: 49, Reward: 10, avg loss: 4.538, eps: 0.647\n",
      "Episode: 50, Reward: 33, avg loss: 5.972, eps: 0.636\n",
      "Episode: 51, Reward: 13, avg loss: 9.225, eps: 0.632\n",
      "Episode: 52, Reward: 17, avg loss: 6.840, eps: 0.626\n",
      "Episode: 53, Reward: 20, avg loss: 5.289, eps: 0.620\n",
      "Episode: 54, Reward: 9, avg loss: 3.863, eps: 0.617\n",
      "Episode: 55, Reward: 15, avg loss: 4.304, eps: 0.612\n",
      "Episode: 56, Reward: 12, avg loss: 4.671, eps: 0.608\n",
      "Episode: 57, Reward: 29, avg loss: 5.427, eps: 0.599\n",
      "Episode: 58, Reward: 8, avg loss: 3.395, eps: 0.596\n",
      "Episode: 59, Reward: 20, avg loss: 4.489, eps: 0.590\n",
      "Episode: 60, Reward: 19, avg loss: 3.748, eps: 0.584\n",
      "Episode: 61, Reward: 27, avg loss: 3.292, eps: 0.576\n",
      "Episode: 62, Reward: 32, avg loss: 2.428, eps: 0.567\n",
      "Episode: 63, Reward: 38, avg loss: 2.684, eps: 0.556\n",
      "Episode: 64, Reward: 14, avg loss: 3.492, eps: 0.552\n",
      "Episode: 65, Reward: 24, avg loss: 2.420, eps: 0.546\n",
      "Episode: 66, Reward: 16, avg loss: 2.183, eps: 0.541\n",
      "Episode: 67, Reward: 29, avg loss: 2.564, eps: 0.533\n",
      "Episode: 68, Reward: 18, avg loss: 1.805, eps: 0.528\n",
      "Episode: 69, Reward: 19, avg loss: 3.618, eps: 0.523\n",
      "Episode: 70, Reward: 17, avg loss: 2.650, eps: 0.518\n",
      "Episode: 71, Reward: 30, avg loss: 2.763, eps: 0.511\n",
      "Episode: 72, Reward: 45, avg loss: 2.192, eps: 0.499\n",
      "Episode: 73, Reward: 53, avg loss: 2.470, eps: 0.486\n",
      "Episode: 74, Reward: 41, avg loss: 2.758, eps: 0.476\n",
      "Episode: 75, Reward: 80, avg loss: 2.284, eps: 0.458\n",
      "Episode: 76, Reward: 66, avg loss: 2.304, eps: 0.443\n",
      "Episode: 77, Reward: 33, avg loss: 2.275, eps: 0.436\n",
      "Episode: 78, Reward: 42, avg loss: 2.008, eps: 0.427\n",
      "Episode: 79, Reward: 23, avg loss: 2.440, eps: 0.422\n",
      "Episode: 80, Reward: 46, avg loss: 2.520, eps: 0.412\n",
      "Episode: 81, Reward: 48, avg loss: 2.179, eps: 0.402\n",
      "Episode: 82, Reward: 54, avg loss: 2.015, eps: 0.392\n",
      "Episode: 83, Reward: 71, avg loss: 2.393, eps: 0.378\n",
      "Episode: 84, Reward: 42, avg loss: 2.082, eps: 0.370\n",
      "Episode: 85, Reward: 87, avg loss: 2.347, eps: 0.355\n",
      "Episode: 86, Reward: 46, avg loss: 1.696, eps: 0.347\n",
      "Episode: 87, Reward: 132, avg loss: 2.304, eps: 0.325\n",
      "Episode: 88, Reward: 68, avg loss: 2.056, eps: 0.315\n",
      "Episode: 89, Reward: 62, avg loss: 2.158, eps: 0.305\n",
      "Episode: 90, Reward: 71, avg loss: 2.010, eps: 0.295\n",
      "Episode: 91, Reward: 80, avg loss: 1.966, eps: 0.283\n",
      "Episode: 92, Reward: 119, avg loss: 2.022, eps: 0.267\n",
      "Episode: 93, Reward: 110, avg loss: 1.738, eps: 0.254\n",
      "Episode: 94, Reward: 65, avg loss: 1.362, eps: 0.246\n",
      "Episode: 95, Reward: 119, avg loss: 1.567, eps: 0.232\n",
      "Episode: 96, Reward: 78, avg loss: 1.431, eps: 0.223\n",
      "Episode: 97, Reward: 71, avg loss: 1.408, eps: 0.216\n",
      "Episode: 98, Reward: 67, avg loss: 1.326, eps: 0.209\n",
      "Episode: 99, Reward: 93, avg loss: 1.226, eps: 0.200\n",
      "Episode: 100, Reward: 83, avg loss: 1.128, eps: 0.192\n",
      "Episode: 101, Reward: 155, avg loss: 1.131, eps: 0.178\n",
      "Episode: 102, Reward: 130, avg loss: 0.966, eps: 0.168\n",
      "Episode: 103, Reward: 95, avg loss: 1.072, eps: 0.160\n",
      "Episode: 104, Reward: 164, avg loss: 0.910, eps: 0.148\n",
      "Episode: 105, Reward: 94, avg loss: 0.946, eps: 0.142\n",
      "Episode: 106, Reward: 161, avg loss: 0.912, eps: 0.132\n",
      "Episode: 107, Reward: 212, avg loss: 0.867, eps: 0.119\n",
      "Episode: 108, Reward: 103, avg loss: 0.849, eps: 0.114\n",
      "Episode: 109, Reward: 154, avg loss: 0.784, eps: 0.106\n",
      "Episode: 110, Reward: 108, avg loss: 0.858, eps: 0.101\n",
      "Episode: 111, Reward: 137, avg loss: 0.862, eps: 0.095\n",
      "Episode: 112, Reward: 194, avg loss: 0.786, eps: 0.087\n",
      "Episode: 113, Reward: 139, avg loss: 0.720, eps: 0.082\n",
      "Episode: 114, Reward: 85, avg loss: 0.816, eps: 0.079\n",
      "Episode: 115, Reward: 124, avg loss: 0.750, eps: 0.075\n",
      "Episode: 116, Reward: 225, avg loss: 0.759, eps: 0.068\n",
      "Episode: 117, Reward: 153, avg loss: 0.824, eps: 0.063\n",
      "Episode: 118, Reward: 192, avg loss: 0.759, eps: 0.059\n",
      "Episode: 119, Reward: 150, avg loss: 0.812, eps: 0.055\n",
      "Episode: 120, Reward: 197, avg loss: 0.758, eps: 0.051\n",
      "Episode: 121, Reward: 190, avg loss: 0.708, eps: 0.047\n",
      "Episode: 122, Reward: 232, avg loss: 0.728, eps: 0.043\n",
      "Episode: 123, Reward: 170, avg loss: 0.751, eps: 0.040\n",
      "Episode: 124, Reward: 245, avg loss: 0.736, eps: 0.037\n",
      "Episode: 125, Reward: 130, avg loss: 0.766, eps: 0.035\n",
      "Episode: 126, Reward: 131, avg loss: 0.757, eps: 0.033\n",
      "Episode: 127, Reward: 228, avg loss: 0.727, eps: 0.031\n",
      "Episode: 128, Reward: 95, avg loss: 0.697, eps: 0.030\n",
      "Episode: 129, Reward: 226, avg loss: 0.779, eps: 0.028\n",
      "Episode: 130, Reward: 290, avg loss: 0.739, eps: 0.025\n",
      "Episode: 131, Reward: 241, avg loss: 0.700, eps: 0.024\n",
      "Episode: 132, Reward: 192, avg loss: 0.698, eps: 0.022\n",
      "Episode: 133, Reward: 190, avg loss: 0.704, eps: 0.021\n",
      "Episode: 134, Reward: 224, avg loss: 0.681, eps: 0.020\n",
      "Episode: 135, Reward: 114, avg loss: 0.700, eps: 0.019\n",
      "Episode: 136, Reward: 256, avg loss: 0.700, eps: 0.018\n",
      "Episode: 137, Reward: 110, avg loss: 0.673, eps: 0.018\n",
      "Episode: 138, Reward: 200, avg loss: 0.676, eps: 0.017\n",
      "Episode: 139, Reward: 213, avg loss: 0.695, eps: 0.016\n",
      "Episode: 140, Reward: 209, avg loss: 0.710, eps: 0.016\n",
      "Episode: 141, Reward: 201, avg loss: 0.686, eps: 0.015\n",
      "Episode: 142, Reward: 141, avg loss: 0.702, eps: 0.015\n",
      "Episode: 143, Reward: 196, avg loss: 0.716, eps: 0.014\n",
      "Episode: 144, Reward: 180, avg loss: 0.667, eps: 0.014\n",
      "Episode: 145, Reward: 191, avg loss: 0.668, eps: 0.014\n",
      "Episode: 146, Reward: 408, avg loss: 0.702, eps: 0.013\n",
      "Episode: 147, Reward: 209, avg loss: 0.656, eps: 0.013\n",
      "Episode: 148, Reward: 126, avg loss: 0.654, eps: 0.013\n",
      "Episode: 149, Reward: 167, avg loss: 0.674, eps: 0.012\n",
      "Episode: 150, Reward: 92, avg loss: 0.675, eps: 0.012\n",
      "Episode: 151, Reward: 249, avg loss: 0.673, eps: 0.012\n",
      "Episode: 152, Reward: 221, avg loss: 0.679, eps: 0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 153, Reward: 173, avg loss: 0.685, eps: 0.012\n",
      "Episode: 154, Reward: 215, avg loss: 0.650, eps: 0.011\n",
      "Episode: 155, Reward: 185, avg loss: 0.654, eps: 0.011\n",
      "Episode: 156, Reward: 180, avg loss: 0.644, eps: 0.011\n",
      "Episode: 157, Reward: 239, avg loss: 0.672, eps: 0.011\n",
      "Episode: 158, Reward: 198, avg loss: 0.682, eps: 0.011\n",
      "Episode: 159, Reward: 100, avg loss: 0.656, eps: 0.011\n",
      "Episode: 160, Reward: 240, avg loss: 0.654, eps: 0.011\n",
      "Episode: 161, Reward: 259, avg loss: 0.653, eps: 0.011\n",
      "Episode: 162, Reward: 187, avg loss: 0.647, eps: 0.011\n",
      "Episode: 163, Reward: 173, avg loss: 0.659, eps: 0.011\n",
      "Episode: 164, Reward: 153, avg loss: 0.621, eps: 0.011\n",
      "Episode: 165, Reward: 106, avg loss: 0.673, eps: 0.011\n",
      "Episode: 166, Reward: 193, avg loss: 0.639, eps: 0.010\n",
      "Episode: 167, Reward: 103, avg loss: 0.619, eps: 0.010\n",
      "Episode: 168, Reward: 240, avg loss: 0.636, eps: 0.010\n",
      "Episode: 169, Reward: 195, avg loss: 0.657, eps: 0.010\n",
      "Episode: 170, Reward: 181, avg loss: 0.653, eps: 0.010\n",
      "Episode: 171, Reward: 183, avg loss: 0.627, eps: 0.010\n",
      "Episode: 172, Reward: 118, avg loss: 0.589, eps: 0.010\n",
      "Episode: 173, Reward: 235, avg loss: 0.665, eps: 0.010\n",
      "Episode: 174, Reward: 200, avg loss: 0.619, eps: 0.010\n",
      "Episode: 175, Reward: 216, avg loss: 0.644, eps: 0.010\n",
      "Episode: 176, Reward: 106, avg loss: 0.664, eps: 0.010\n",
      "Episode: 177, Reward: 100, avg loss: 0.653, eps: 0.010\n",
      "Episode: 178, Reward: 92, avg loss: 0.647, eps: 0.010\n",
      "Episode: 179, Reward: 209, avg loss: 0.628, eps: 0.010\n",
      "Episode: 180, Reward: 166, avg loss: 0.621, eps: 0.010\n",
      "Episode: 181, Reward: 105, avg loss: 0.627, eps: 0.010\n",
      "Episode: 182, Reward: 129, avg loss: 0.639, eps: 0.010\n",
      "Episode: 183, Reward: 197, avg loss: 0.649, eps: 0.010\n",
      "Episode: 184, Reward: 237, avg loss: 0.637, eps: 0.010\n",
      "Episode: 185, Reward: 112, avg loss: 0.600, eps: 0.010\n",
      "Episode: 186, Reward: 266, avg loss: 0.670, eps: 0.010\n",
      "Episode: 187, Reward: 120, avg loss: 0.664, eps: 0.010\n",
      "Episode: 188, Reward: 127, avg loss: 0.699, eps: 0.010\n",
      "Episode: 189, Reward: 139, avg loss: 0.647, eps: 0.010\n",
      "Episode: 190, Reward: 205, avg loss: 0.720, eps: 0.010\n",
      "Episode: 191, Reward: 149, avg loss: 0.638, eps: 0.010\n",
      "Episode: 192, Reward: 204, avg loss: 0.668, eps: 0.010\n",
      "Episode: 193, Reward: 236, avg loss: 0.686, eps: 0.010\n",
      "Episode: 194, Reward: 266, avg loss: 0.693, eps: 0.010\n",
      "Episode: 195, Reward: 268, avg loss: 0.704, eps: 0.010\n",
      "Episode: 196, Reward: 292, avg loss: 0.654, eps: 0.010\n",
      "Episode: 197, Reward: 162, avg loss: 0.711, eps: 0.010\n",
      "Episode: 198, Reward: 190, avg loss: 0.734, eps: 0.010\n",
      "Episode: 199, Reward: 202, avg loss: 0.784, eps: 0.010\n",
      "Episode: 200, Reward: 179, avg loss: 0.736, eps: 0.010\n",
      "Episode: 201, Reward: 235, avg loss: 0.773, eps: 0.010\n",
      "Episode: 202, Reward: 187, avg loss: 0.783, eps: 0.010\n",
      "Episode: 203, Reward: 172, avg loss: 0.706, eps: 0.010\n",
      "Episode: 204, Reward: 153, avg loss: 0.764, eps: 0.010\n",
      "Episode: 205, Reward: 152, avg loss: 0.781, eps: 0.010\n",
      "Episode: 206, Reward: 176, avg loss: 0.724, eps: 0.010\n",
      "Episode: 207, Reward: 187, avg loss: 0.762, eps: 0.010\n",
      "Episode: 208, Reward: 194, avg loss: 0.701, eps: 0.010\n",
      "Episode: 209, Reward: 279, avg loss: 0.775, eps: 0.010\n",
      "Episode: 210, Reward: 253, avg loss: 0.741, eps: 0.010\n",
      "Episode: 211, Reward: 219, avg loss: 0.753, eps: 0.010\n",
      "Episode: 212, Reward: 163, avg loss: 0.727, eps: 0.010\n",
      "Episode: 213, Reward: 179, avg loss: 0.703, eps: 0.010\n",
      "Episode: 214, Reward: 184, avg loss: 0.661, eps: 0.010\n",
      "Episode: 215, Reward: 209, avg loss: 0.695, eps: 0.010\n",
      "Episode: 216, Reward: 220, avg loss: 0.690, eps: 0.010\n",
      "Episode: 217, Reward: 193, avg loss: 0.666, eps: 0.010\n",
      "Episode: 218, Reward: 201, avg loss: 0.741, eps: 0.010\n",
      "Episode: 219, Reward: 182, avg loss: 0.767, eps: 0.010\n",
      "Episode: 220, Reward: 187, avg loss: 0.719, eps: 0.010\n",
      "Episode: 221, Reward: 223, avg loss: 0.692, eps: 0.010\n",
      "Episode: 222, Reward: 176, avg loss: 0.733, eps: 0.010\n",
      "Episode: 223, Reward: 205, avg loss: 0.707, eps: 0.010\n",
      "Episode: 224, Reward: 197, avg loss: 0.685, eps: 0.010\n",
      "Episode: 225, Reward: 258, avg loss: 0.672, eps: 0.010\n",
      "Episode: 226, Reward: 183, avg loss: 0.666, eps: 0.010\n",
      "Episode: 227, Reward: 219, avg loss: 0.672, eps: 0.010\n",
      "Episode: 228, Reward: 221, avg loss: 0.671, eps: 0.010\n",
      "Episode: 229, Reward: 253, avg loss: 0.727, eps: 0.010\n",
      "Episode: 230, Reward: 193, avg loss: 0.687, eps: 0.010\n",
      "Episode: 231, Reward: 230, avg loss: 0.682, eps: 0.010\n",
      "Episode: 232, Reward: 152, avg loss: 0.661, eps: 0.010\n",
      "Episode: 233, Reward: 280, avg loss: 0.718, eps: 0.010\n",
      "Episode: 234, Reward: 165, avg loss: 0.649, eps: 0.010\n",
      "Episode: 235, Reward: 175, avg loss: 0.656, eps: 0.010\n",
      "Episode: 236, Reward: 261, avg loss: 0.687, eps: 0.010\n",
      "Episode: 237, Reward: 190, avg loss: 0.695, eps: 0.010\n",
      "Episode: 238, Reward: 253, avg loss: 0.666, eps: 0.010\n",
      "Episode: 239, Reward: 204, avg loss: 0.669, eps: 0.010\n",
      "Episode: 240, Reward: 170, avg loss: 0.654, eps: 0.010\n",
      "Episode: 241, Reward: 206, avg loss: 0.659, eps: 0.010\n",
      "Episode: 242, Reward: 292, avg loss: 0.682, eps: 0.010\n",
      "Episode: 243, Reward: 196, avg loss: 0.693, eps: 0.010\n",
      "Episode: 244, Reward: 170, avg loss: 0.650, eps: 0.010\n",
      "Episode: 245, Reward: 288, avg loss: 0.645, eps: 0.010\n",
      "Episode: 246, Reward: 202, avg loss: 0.726, eps: 0.010\n",
      "Episode: 247, Reward: 203, avg loss: 0.616, eps: 0.010\n",
      "Episode: 248, Reward: 213, avg loss: 0.697, eps: 0.010\n",
      "Episode: 249, Reward: 241, avg loss: 0.688, eps: 0.010\n",
      "Episode: 250, Reward: 168, avg loss: 0.631, eps: 0.010\n",
      "Episode: 251, Reward: 302, avg loss: 0.649, eps: 0.010\n",
      "Episode: 252, Reward: 175, avg loss: 0.686, eps: 0.010\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "STORE_PATH = '.'\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "LAMBDA = 0.0005\n",
    "GAMMA = 0.95\n",
    "BATCH_SIZE = 32\n",
    "TAU = 0.08\n",
    "RANDOM_REWARD_STD = 1.0\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = 4\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(num_actions)\n",
    "])\n",
    "\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer=keras.initializers.he_normal()),\n",
    "    keras.layers.Dense(num_actions)\n",
    "])\n",
    "\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "\n",
    "# print('model init weight',primary_network.get_weights())\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "\n",
    "memory = Memory(500000)\n",
    "\n",
    "\n",
    "def choose_action(state, primary_network, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "#         print(\"\\n\\n\\n choose_action      state\", state.shape, state.reshape(1, -1).shape)\n",
    "        return np.argmax(primary_network(state.reshape(1, -1)))\n",
    "\n",
    "\n",
    "def train(primary_network, memory, target_network=None):\n",
    "    if memory.num_samples < BATCH_SIZE * 3:\n",
    "        return 0\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch])\n",
    "    \n",
    "    \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([(np.zeros(state_size)\n",
    "                             if val[3] is None else val[3]) for val in batch])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\"\\n batch\", len(batch), batch[0])\n",
    "    \n",
    "    \n",
    "#     print(\"\\n states\", states.shape)\n",
    "#     print(\"\\n actions\", actions.shape)\n",
    "#     print(\"\\n rewards\", rewards.shape)\n",
    "#     print(\"\\n next_states\", next_states.shape)\n",
    "    \n",
    "    \n",
    "    # predict Q(s,a) given the batch of states\n",
    "\n",
    "    prim_qt = primary_network(states)\n",
    "#     print(\"\\n prim_qt\", prim_qt.shape)\n",
    "    \n",
    "    # predict Q(s',a') from the evaluation network\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    \n",
    "#     print(\"\\n prim_qtp1\", prim_qtp1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "    target_q = prim_qt.numpy()\n",
    "    \n",
    "#     print(\"\\n\\n\\n\\n target_q\", target_q.shape, target_q)\n",
    "    \n",
    "    \n",
    "    \n",
    "    updates = rewards\n",
    "    valid_idxs = np.array(next_states).sum(axis=1) != 0\n",
    "    \n",
    "    \n",
    "    batch_idxs = np.arange(BATCH_SIZE)    \n",
    "    \n",
    "    \n",
    "\n",
    "    if target_network is None:\n",
    "        updates[valid_idxs] += GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1)\n",
    "    else:\n",
    "        \n",
    "        \n",
    "#         print(\"\\prim_qtp1\", prim_qtp1.numpy().shape)\n",
    "        \n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "#         print(\"prim_action_tp1\", prim_action_tp1.shape)\n",
    "        \n",
    "        q_from_target = target_network(next_states)\n",
    "#         print(\"q_from_target\", q_from_target.shape)\n",
    "        \n",
    "        \n",
    "#         print(\"batch_idxs[valid_idxs]\", batch_idxs[valid_idxs].shape, batch_idxs[valid_idxs][0:2])\n",
    "#         print(\"prim_action_tp1[valid_idxs]\", prim_action_tp1[valid_idxs].shape, prim_action_tp1[valid_idxs][0:2])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"gamma * ...\", q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]].shape)\n",
    "        \n",
    "#         print(\"\\n updates[valid_idxs]\", updates[valid_idxs].shape)\n",
    "        \n",
    "        updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "#     print(\"\\n\\n actions\", actions.shape, actions)\n",
    "    \n",
    "#     print(\"\\n\\n batch_idxs\", batch_idxs.shape, batch_idxs)\n",
    "    \n",
    "    \n",
    "#     print(\"\\n\\n  target_q[batch_idxs, actions]\", target_q[batch_idxs, actions].shape, target_q[batch_idxs, actions])\n",
    "    target_q[batch_idxs, actions] = updates\n",
    "#     print(\"\\n\\n\\n target_q\", target_q.shape)\n",
    "\n",
    "    \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    \n",
    "    if target_network is not None:\n",
    "        # update target network parameters slowly from primary network\n",
    "        for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "            t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "    return loss\n",
    "\n",
    "num_episodes = 1000\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/DoubleQ_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "double_q = True\n",
    "steps = 0\n",
    "\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    cnt = 0\n",
    "    avg_loss = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "\n",
    "        action = choose_action(state, primary_network, eps)\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = np.random.normal(1.0, RANDOM_REWARD_STD)\n",
    "        \n",
    "                \n",
    "        if done:\n",
    "            next_state = None\n",
    "        # store in memory\n",
    "        memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "        loss = train(primary_network, memory, target_network if double_q else None)\n",
    "        avg_loss += loss\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # exponentially decay the eps value\n",
    "        steps += 1\n",
    "        eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * steps)\n",
    "\n",
    "        if done:\n",
    "            avg_loss /= cnt\n",
    "            print(f\"Episode: {i}, Reward: {cnt}, avg loss: {avg_loss:.3f}, eps: {eps:.3f}\")\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('reward', cnt, step=i)\n",
    "                tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            break\n",
    "\n",
    "        cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-24T14:04:42.072Z"
    }
   },
   "outputs": [],
   "source": [
    "render = True\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    cnt = 0\n",
    "    avg_loss = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        eps = 0.\n",
    "        action = choose_action(state, primary_network, eps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = np.random.normal(1.0, RANDOM_REWARD_STD)\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            avg_loss /= cnt\n",
    "            print(f\"Episode: {i}, Reward: {cnt}, avg loss: {avg_loss:.3f}, eps: {eps:.3f}\")\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('reward', cnt, step=i)\n",
    "                tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            break\n",
    "\n",
    "        cnt += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-24T14:04:42.075Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
